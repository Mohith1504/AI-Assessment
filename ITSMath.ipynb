{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "metadata": {
        "id": "wGS2uIwfTQtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "CaTe8uAETQwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/2012-2013-data-with-predictions-4-final.csv')\n"
      ],
      "metadata": {
        "id": "V1ASDqeuTQyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "e7afd770-c57d-4912-bf10-cd89cf8d77bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 2022458",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8bd636aeb17b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/2012-2013-data-with-predictions-4-final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 2022458"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJ0SW7T9ZNxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "4rysc8CATWQ3",
        "outputId": "343315ad-638d-4ed1-c7d2-5e955872041c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a475d982b3d9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "Sv7i_8WJTQ10",
        "outputId": "7c734ed0-9b3a-4178-b8d7-b3b8acc71ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-00cf07b74dcd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "HGw-HsiBTbLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "irrelevant_columns = [\n",
        "    'problem_log_id', 'problem_id', 'user_id', 'teacher_id',\n",
        "    'school_id', 'answer_id', 'answer_text', 'actions', 'tutor_mode', 'skill'\n",
        "]\n",
        "\n",
        "# Drop them from the dataset\n",
        "data = df.drop(columns=irrelevant_columns)"
      ],
      "metadata": {
        "id": "5To0PYdzTb7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "Rg9wj8tcTb-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "HCtjubX7TcBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "VjK3gd8TTiVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "_v7l6x1sTiX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['start_time'] = data['start_time'].str.split('.').str[0]\n",
        "data['end_time'] = data['end_time'].str.split('.').str[0]\n",
        "\n",
        "# Then, convert to datetime\n",
        "data['start_time'] = pd.to_datetime(data['start_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "data['end_time'] = pd.to_datetime(data['end_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Calculate 'time_taken' in seconds\n",
        "data['time_taken'] = (data['end_time'] - data['start_time']).dt.total_seconds()\n",
        "\n",
        "# Convert to float\n",
        "data['time_taken'] = data['time_taken'].astype(float)\n",
        "\n",
        "# Drop the original datetime columns\n",
        "data = data.drop(columns=['start_time', 'end_time'])\n",
        "\n",
        "# Check the result\n",
        "print(data[['time_taken']].head())\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = ['skill_id', 'problem_type', 'first_action', 'type']\n",
        "\n",
        "# Apply Label Encoding\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    data[col] = label_encoders[col].fit_transform(data[col])\n",
        "\n",
        "print(data.dtypes)"
      ],
      "metadata": {
        "id": "GxFfaDiDTiex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming your dataset is `data` and your target column is 'correct'\n",
        "X = data.drop(columns=['correct'])  # Features\n",
        "y = data['correct']  # Target\n",
        "y = y.astype(int)\n",
        "\n",
        "# Stratified sampling to ensure equal class distribution\n",
        "X_sampled, X_test, y_sampled, y_test = train_test_split(X, y, test_size=0.99, stratify=y, random_state=42)\n",
        "\n",
        "# Train a Random Forest model to rank features\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_sampled, y_sampled)\n",
        "\n",
        "# Plot feature importance\n",
        "importance = pd.Series(model.feature_importances_, index=X.columns)\n",
        "importance.sort_values(ascending=False).plot(kind='bar', figsize=(10, 6), title='Feature Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ge-nzh-uUYOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "YrmQ8nV4UYTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_test and y_pred are already defined\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Optionally, visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=set(y_test))\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ddrXooxpUoDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your dataset is `data` and your target column is 'correct'\n",
        "X = data[['attempt_count', 'hint_count', 'bottom_hint', 'time_taken', 'first_action', 'ms_first_response', 'overlap_time',\n",
        "          'Average_confidence(CONCENTRATING)', 'Average_confidence(BORED)']] # Features\n",
        "y = data['correct']  # Target\n",
        "y = y.astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "cWW8F0uqUoGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Conv1D, MaxPooling1D, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "TRk97WMnUoJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "num_classes = y.nunique()\n",
        "\n",
        "# Convert labels to categorical for neural network use\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# Define models and hyperparameter grids for grid search\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'Linear Regression': {},  # No hyperparameters for basic linear regression\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance']\n",
        "    },\n",
        "    'Naive Bayes': {},  # No hyperparameters typically tuned\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30]\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [None, 5, 10, 15],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Placeholder for results\n",
        "results = []\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    if param_grids[model_name]:  # If hyperparameters are defined\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring='f1_weighted', cv=5)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "    else:\n",
        "        best_model = model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    if y_pred.dtype != int and model_name != 'Linear Regression':\n",
        "      y_pred = (y_pred > 0.5).astype(int)\n",
        "    if model_name != 'Linear Regression':  # Skip non-classification metrics for linear regression\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        results.append({'Model': model_name, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1})\n",
        "\n",
        "# Neural Network models\n",
        "def create_rnn_model(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=input_dim, output_dim=128),\n",
        "        SimpleRNN(64),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_lstm_model(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=input_dim, output_dim=128),\n",
        "        LSTM(64),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_capsule_network(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=input_dim, output_dim=128),\n",
        "        Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "neural_networks = {\n",
        "    'RNN': create_rnn_model(input_dim=10000, num_classes=num_classes),\n",
        "    'LSTM': create_lstm_model(input_dim=10000, num_classes=num_classes),\n",
        "    'Capsule Network': create_capsule_network(input_dim=10000, num_classes=num_classes)\n",
        "}\n",
        "\n",
        "for name, model in neural_networks.items():\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
        "    model.fit(X_train, y_train_cat, epochs=10, batch_size=64)\n",
        "    metrics = model.evaluate(X_test, y_test_cat)\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': metrics[1],\n",
        "        'Precision': metrics[2],\n",
        "        'Recall': metrics[3],\n",
        "        'F1 Score': 2 * (metrics[2] * metrics[3]) / (metrics[2] + metrics[3])  # Harmonic mean of precision and recall\n",
        "    })\n",
        "\n",
        "# Convert results to a DataFrame for display\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display results\n",
        "print(results_df.sort_values(by='F1 Score', ascending=False))\n",
        "\n",
        "# Plot F1 scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(results_df['Model'], results_df['F1 Score'], color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('F1 Score', fontsize=12)\n",
        "plt.title('F1 Score Comparison of Models', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the results DataFrame\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "QgAPyRjYVDu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "zAe2tVeoVD3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Identify weak skills for each student\n",
        "incorrect_answers = df[df['correct'] == 0]\n",
        "\n",
        "# Group by `user_id` and aggregate their weak `skill_id`\n",
        "student_weak_skills = incorrect_answers.groupby('user_id')['skill_id'].apply(\n",
        "    lambda x: ', '.join(map(str, x.dropna().unique()))\n",
        ").reset_index()\n",
        "student_weak_skills.rename(columns={'skill_id': 'weak_skills'}, inplace=True)\n",
        "\n",
        "# Step 2: Prepare the resources (problems with skill associations)\n",
        "resource_skills = df[['problem_id', 'skill_id']].drop_duplicates()\n",
        "resource_skills['skill_id'] = resource_skills['skill_id'].astype(str)\n",
        "\n",
        "# Step 3: Combine skills data for TF-IDF vectorization\n",
        "combined_skills = pd.concat([\n",
        "    student_weak_skills['weak_skills'],\n",
        "    resource_skills['skill_id']\n",
        "], axis=0).fillna('')\n",
        "\n",
        "# Vectorize skill data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "skills_tfidf = vectorizer.fit_transform(combined_skills)\n",
        "\n",
        "# Step 4: Compute similarity\n",
        "# Split back into student and resource vectors\n",
        "student_skills_tfidf = skills_tfidf[:len(student_weak_skills)]\n",
        "resource_skills_tfidf = skills_tfidf[len(student_weak_skills):]\n",
        "\n",
        "similarity_matrix = cosine_similarity(student_skills_tfidf, resource_skills_tfidf)\n",
        "\n",
        "# Step 5: Create a recommendation function\n",
        "def recommend_resources(student_id, top_n=5):\n",
        "    # Check if the student exists in the weak_skills data\n",
        "    if student_id not in student_weak_skills['user_id'].values:\n",
        "        print(f\"Student ID {student_id} not found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Get the student's index in the similarity matrix\n",
        "    student_index = student_weak_skills[student_weak_skills['user_id'] == student_id].index[0]\n",
        "    similarity_scores = list(enumerate(similarity_matrix[student_index]))\n",
        "\n",
        "    # Rank resources by similarity scores\n",
        "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "    top_resource_indices = [i[0] for i in similarity_scores[:top_n]]\n",
        "    recommended_resources = resource_skills.iloc[top_resource_indices]\n",
        "\n",
        "    return recommended_resources\n",
        "\n",
        "# Step 6: Test the recommender system\n",
        "student_id = 52535  # Replace with an actual `user_id` from the dataset\n",
        "recommendations = recommend_resources(student_id, top_n=5)\n",
        "\n",
        "# Map skill IDs to skill names\n",
        "skill_mapping = {85.0: \"Algebra\"}  # Ensure this dictionary includes all relevant skill IDs\n",
        "\n",
        "# Avoid SettingWithCopyWarning by making a copy\n",
        "recommendations = recommendations.copy()\n",
        "\n",
        "# Ensure matching data types\n",
        "recommendations.loc[:, 'skill_id'] = recommendations['skill_id'].astype(float)\n",
        "\n",
        "# Map skill IDs and handle missing mappings\n",
        "recommendations.loc[:, 'skill_name'] = recommendations['skill_id'].map(skill_mapping).fillna(\"Unknown Skill\")\n",
        "\n",
        "# Display recommendations with skill names\n",
        "print(recommendations[['problem_id', 'skill_name']])\n",
        "\n",
        "# Visualization: Plot recommendations\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(recommendations['problem_id'].astype(str), recommendations['skill_name'])\n",
        "plt.xlabel('Problem ID', fontsize=12)\n",
        "plt.ylabel('Skill Name', fontsize=12)\n",
        "plt.title(f'Recommended Problems for Student {student_id}', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ix8T9mN8VD7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iTGZgW_QVbyt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}